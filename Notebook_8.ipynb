{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8f25f2e-3587-4da0-9d61-e64ecc3dd86c",
   "metadata": {},
   "source": [
    "Execution Query Plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5dbac84-3c8a-461a-a840-a4f1572f2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, col, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "390e8efe-245b-440d-8dec-80323437a671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/21 07:12:34 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "             .master(\"local[1]\")\\\n",
    "             .appName(\"test\")\\\n",
    "             .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1094615d-264f-4312-9317-08bda4c9c784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read non-partitioned taxi data\n",
    "local_files = '/home/sasa/Downloads/Code/notebooks/datasets/parquet/'\n",
    "df_taxis_non_partitioned_raw = spark.read.parquet(local_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bcf0fff-d67d-471e-b752-56748ecc42da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we cleaned the data in the previous notebook, let's do the same:\n",
    "df_taxis_non_partitioned_raw = df_taxis_non_partitioned_raw.where(year(col('tpep_pickup_datetime')) == '2023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75dde321-6eb4-465a-8bee-00555a865c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read partitioned taxi data\n",
    "local_path = '/home/sasa/Downloads/Code/notebooks/datasets/yellow_taxis_daily/'\n",
    "df_taxis_daily_raw = spark.read.parquet(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b50456d-41cf-45fd-a601-75805460e78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      " |-- p_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show schema and find new partition column\n",
    "df_taxis_daily_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d4cdf3c-788e-47e3-807d-225ad1468942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|    p_date|\n",
      "+----------+\n",
      "|2023-04-14|\n",
      "|2023-04-14|\n",
      "|2023-04-14|\n",
      "+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show new partition column\n",
    "df_taxis_daily_raw.select('p_date').show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8378a4ca-6ac0-4891-989a-51e242f0a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create same column p_date, so we can compare plans\n",
    "df_taxis_nopartitioned_raw = df_taxis_non_partitioned_raw.withColumn(\"p_date\",to_date(col('tpep_pickup_datetime')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82a63159-7bb9-4e99-ae73-00b9281b60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Non-partitioned DF as View\n",
    "df_taxis_nopartitioned_raw.createOrReplaceTempView(\"tbl_taxis_nopartitioned_raw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3c7138a-e565-42fc-a659-0f68f0c14e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Daily DF as View\n",
    "df_taxis_daily_raw.createOrReplaceTempView(\"tbl_taxis_daily_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "248a69c2-6b8c-4c8d-bf14-6c67a49f55d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query by partition Key; i.e. using '2023-02-14' as filter\n",
    "q1a = spark.sql(\"select avg(trip_distance) from tbl_taxis_daily_raw where p_date='2023-02-14' and RatecodeID=2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1e60f62-63f0-4d5d-9aed-722fe05a2401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|avg(trip_distance)|\n",
      "+------------------+\n",
      "|17.138035006604998|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show data\n",
    "q1a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e82e1e1-e35b-42eb-8a6d-9d040a43da00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('avg('trip_distance), None)]\n",
      "+- 'Filter (('p_date = 2023-02-14) AND ('RatecodeID = 2))\n",
      "   +- 'UnresolvedRelation [tbl_taxis_daily_raw], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "avg(trip_distance): double\n",
      "Aggregate [avg(trip_distance#43) AS avg(trip_distance)#108]\n",
      "+- Filter ((p_date#58 = cast(2023-02-14 as date)) AND (RatecodeID#44L = cast(2 as bigint)))\n",
      "   +- SubqueryAlias tbl_taxis_daily_raw\n",
      "      +- View (`tbl_taxis_daily_raw`, [VendorID#39,tpep_pickup_datetime#40,tpep_dropoff_datetime#41,passenger_count#42L,trip_distance#43,RatecodeID#44L,store_and_fwd_flag#45,PULocationID#46,DOLocationID#47,payment_type#48L,fare_amount#49,extra#50,mta_tax#51,tip_amount#52,tolls_amount#53,improvement_surcharge#54,total_amount#55,congestion_surcharge#56,Airport_fee#57,p_date#58])\n",
      "         +- Relation [VendorID#39,tpep_pickup_datetime#40,tpep_dropoff_datetime#41,passenger_count#42L,trip_distance#43,RatecodeID#44L,store_and_fwd_flag#45,PULocationID#46,DOLocationID#47,payment_type#48L,fare_amount#49,extra#50,mta_tax#51,tip_amount#52,tolls_amount#53,improvement_surcharge#54,total_amount#55,congestion_surcharge#56,Airport_fee#57,p_date#58] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [avg(trip_distance#43) AS avg(trip_distance)#108]\n",
      "+- Project [trip_distance#43]\n",
      "   +- Filter ((isnotnull(p_date#58) AND isnotnull(RatecodeID#44L)) AND ((p_date#58 = 2023-02-14) AND (RatecodeID#44L = 2)))\n",
      "      +- Relation [VendorID#39,tpep_pickup_datetime#40,tpep_dropoff_datetime#41,passenger_count#42L,trip_distance#43,RatecodeID#44L,store_and_fwd_flag#45,PULocationID#46,DOLocationID#47,payment_type#48L,fare_amount#49,extra#50,mta_tax#51,tip_amount#52,tolls_amount#53,improvement_surcharge#54,total_amount#55,congestion_surcharge#56,Airport_fee#57,p_date#58] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[avg(trip_distance#43)], output=[avg(trip_distance)#108])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=90]\n",
      "      +- HashAggregate(keys=[], functions=[partial_avg(trip_distance#43)], output=[sum#115, count#116L])\n",
      "         +- Project [trip_distance#43]\n",
      "            +- Filter (isnotnull(RatecodeID#44L) AND (RatecodeID#44L = 2))\n",
      "               +- FileScan parquet [trip_distance#43,RatecodeID#44L,p_date#58] Batched: true, DataFilters: [isnotnull(RatecodeID#44L), (RatecodeID#44L = 2)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/sasa/Downloads/Code/notebooks/datasets/yellow_taxis_daily], PartitionFilters: [isnotnull(p_date#58), (p_date#58 = 2023-02-14)], PushedFilters: [IsNotNull(RatecodeID), EqualTo(RatecodeID,2)], ReadSchema: struct<trip_distance:double,RatecodeID:bigint>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explain plan\n",
    "q1a.explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a38ed8b8-e159-4dde-81ef-fab7532b699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query by partition Key; i.e. using '2023-02-14' as filter\n",
    "q1b = spark.sql(\"select avg(trip_distance) from tbl_taxis_nopartitioned_raw where p_date='2023-02-14' and RatecodeID=2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d41646d-8e24-4f67-af30-a15231d4c3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('avg('trip_distance), None)]\n",
      "+- 'Filter (('p_date = 2023-02-14) AND ('RatecodeID = 2))\n",
      "   +- 'UnresolvedRelation [tbl_taxis_nopartitioned_raw], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "avg(trip_distance): double\n",
      "Aggregate [avg(trip_distance#4) AS avg(trip_distance)#124]\n",
      "+- Filter ((p_date#86 = cast(2023-02-14 as date)) AND (RatecodeID#5L = cast(2 as bigint)))\n",
      "   +- SubqueryAlias tbl_taxis_nopartitioned_raw\n",
      "      +- View (`tbl_taxis_nopartitioned_raw`, [VendorID#0,tpep_pickup_datetime#1,tpep_dropoff_datetime#2,passenger_count#3L,trip_distance#4,RatecodeID#5L,store_and_fwd_flag#6,PULocationID#7,DOLocationID#8,payment_type#9L,fare_amount#10,extra#11,mta_tax#12,tip_amount#13,tolls_amount#14,improvement_surcharge#15,total_amount#16,congestion_surcharge#17,Airport_fee#18,p_date#86])\n",
      "         +- Project [VendorID#0, tpep_pickup_datetime#1, tpep_dropoff_datetime#2, passenger_count#3L, trip_distance#4, RatecodeID#5L, store_and_fwd_flag#6, PULocationID#7, DOLocationID#8, payment_type#9L, fare_amount#10, extra#11, mta_tax#12, tip_amount#13, tolls_amount#14, improvement_surcharge#15, total_amount#16, congestion_surcharge#17, Airport_fee#18, to_date(tpep_pickup_datetime#1, None, Some(Asia/Kolkata), false) AS p_date#86]\n",
      "            +- Filter (year(cast(tpep_pickup_datetime#1 as date)) = cast(2023 as int))\n",
      "               +- Relation [VendorID#0,tpep_pickup_datetime#1,tpep_dropoff_datetime#2,passenger_count#3L,trip_distance#4,RatecodeID#5L,store_and_fwd_flag#6,PULocationID#7,DOLocationID#8,payment_type#9L,fare_amount#10,extra#11,mta_tax#12,tip_amount#13,tolls_amount#14,improvement_surcharge#15,total_amount#16,congestion_surcharge#17,Airport_fee#18] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [avg(trip_distance#4) AS avg(trip_distance)#124]\n",
      "+- Project [trip_distance#4]\n",
      "   +- Filter ((isnotnull(tpep_pickup_datetime#1) AND isnotnull(RatecodeID#5L)) AND ((year(cast(tpep_pickup_datetime#1 as date)) = 2023) AND (((tpep_pickup_datetime#1 >= 2023-02-14 00:00:00) AND (tpep_pickup_datetime#1 < 2023-02-15 00:00:00)) AND (RatecodeID#5L = 2))))\n",
      "      +- Relation [VendorID#0,tpep_pickup_datetime#1,tpep_dropoff_datetime#2,passenger_count#3L,trip_distance#4,RatecodeID#5L,store_and_fwd_flag#6,PULocationID#7,DOLocationID#8,payment_type#9L,fare_amount#10,extra#11,mta_tax#12,tip_amount#13,tolls_amount#14,improvement_surcharge#15,total_amount#16,congestion_surcharge#17,Airport_fee#18] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[avg(trip_distance#4)], output=[avg(trip_distance)#124])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=107]\n",
      "      +- HashAggregate(keys=[], functions=[partial_avg(trip_distance#4)], output=[sum#128, count#129L])\n",
      "         +- Project [trip_distance#4]\n",
      "            +- Filter (((((isnotnull(tpep_pickup_datetime#1) AND isnotnull(RatecodeID#5L)) AND (year(cast(tpep_pickup_datetime#1 as date)) = 2023)) AND (tpep_pickup_datetime#1 >= 2023-02-14 00:00:00)) AND (tpep_pickup_datetime#1 < 2023-02-15 00:00:00)) AND (RatecodeID#5L = 2))\n",
      "               +- FileScan parquet [tpep_pickup_datetime#1,trip_distance#4,RatecodeID#5L] Batched: true, DataFilters: [isnotnull(tpep_pickup_datetime#1), isnotnull(RatecodeID#5L), (year(cast(tpep_pickup_datetime#1 a..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/sasa/Downloads/Code/notebooks/datasets/parquet], PartitionFilters: [], PushedFilters: [IsNotNull(tpep_pickup_datetime), IsNotNull(RatecodeID), GreaterThanOrEqual(tpep_pickup_datetime,..., ReadSchema: struct<tpep_pickup_datetime:timestamp_ntz,trip_distance:double,RatecodeID:bigint>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explain plan\n",
    "q1b.explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58bdea0d-925a-4dbc-bfae-8eb0b711581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query by partition Key; i.e. using '2023-02-14' as filter\n",
    "q2a = spark.sql(\"select p_date,count(1) from tbl_taxis_daily_raw where p_date in\n",
    "('2023-02-14','2023-02-15','2023-02-16')group by p_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "727a4ff9-98d3-427f-9da4-29752320f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query by partition Key; i.e. using '2023-02-14' as filter\n",
    "q2b = spark.sql(\"select p_date,count(1) from tbl_taxis_nopartitioned_raw where p_date in \n",
    "('2023-02-14','2023-02-15','2023-02-16') group by p_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c6c176f-830f-4242-9149-404235032102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[p_date#58], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(p_date#58, 200), ENSURE_REQUIREMENTS, [plan_id=120]\n",
      "      +- HashAggregate(keys=[p_date#58], functions=[partial_count(1)])\n",
      "         +- FileScan parquet [p_date#58] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/sasa/Downloads/Code/notebooks/datasets/yellow_taxis_daily], PartitionFilters: [cast(p_date#58 as string) IN (2023-02-14,2023-02-15,2023-02-16)], PushedFilters: [], ReadSchema: struct<>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show plan\n",
    "q2a.explain(extended=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cb7ed55-c9b1-4132-bb3e-92ac3c72983d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[p_date#86], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(p_date#86, 200), ENSURE_REQUIREMENTS, [plan_id=137]\n",
      "      +- HashAggregate(keys=[p_date#86], functions=[partial_count(1)])\n",
      "         +- Project [cast(tpep_pickup_datetime#1 as date) AS p_date#86]\n",
      "            +- Filter ((isnotnull(tpep_pickup_datetime#1) AND (year(cast(tpep_pickup_datetime#1 as date)) = 2023)) AND cast(cast(tpep_pickup_datetime#1 as date) as string) IN (2023-02-14,2023-02-15,2023-02-16))\n",
      "               +- FileScan parquet [tpep_pickup_datetime#1] Batched: true, DataFilters: [isnotnull(tpep_pickup_datetime#1), (year(cast(tpep_pickup_datetime#1 as date)) = 2023), cast(cas..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/sasa/Downloads/Code/notebooks/datasets/parquet], PartitionFilters: [], PushedFilters: [IsNotNull(tpep_pickup_datetime)], ReadSchema: struct<tpep_pickup_datetime:timestamp_ntz>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show plan\n",
    "q2b.explain(extended=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f658d5f-f87a-479a-baff-61ea2c8d5f06",
   "metadata": {},
   "source": [
    "#Compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc5df408-af5b-4e3f-a55b-2c356a94b7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (5)\n",
      "+- HashAggregate (4)\n",
      "   +- Exchange (3)\n",
      "      +- HashAggregate (2)\n",
      "         +- Scan parquet  (1)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [1]: [p_date#58]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/home/sasa/Downloads/Code/notebooks/datasets/yellow_taxis_daily]\n",
      "PartitionFilters: [cast(p_date#58 as string) IN (2023-02-14,2023-02-15,2023-02-16)]\n",
      "ReadSchema: struct<>\n",
      "\n",
      "(2) HashAggregate\n",
      "Input [1]: [p_date#58]\n",
      "Keys [1]: [p_date#58]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#138L]\n",
      "Results [2]: [p_date#58, count#139L]\n",
      "\n",
      "(3) Exchange\n",
      "Input [2]: [p_date#58, count#139L]\n",
      "Arguments: hashpartitioning(p_date#58, 200), ENSURE_REQUIREMENTS, [plan_id=120]\n",
      "\n",
      "(4) HashAggregate\n",
      "Input [2]: [p_date#58, count#139L]\n",
      "Keys [1]: [p_date#58]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#130L]\n",
      "Results [2]: [p_date#58, count(1)#130L AS count(1)#131L]\n",
      "\n",
      "(5) AdaptiveSparkPlan\n",
      "Output [2]: [p_date#58, count(1)#131L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show plan\n",
    "q2a.explain(extended=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cb82bc7-3116-4858-b47c-ac2842e30e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (7)\n",
      "+- HashAggregate (6)\n",
      "   +- Exchange (5)\n",
      "      +- HashAggregate (4)\n",
      "         +- Project (3)\n",
      "            +- Filter (2)\n",
      "               +- Scan parquet  (1)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [1]: [tpep_pickup_datetime#1]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/home/sasa/Downloads/Code/notebooks/datasets/parquet]\n",
      "PushedFilters: [IsNotNull(tpep_pickup_datetime)]\n",
      "ReadSchema: struct<tpep_pickup_datetime:timestamp_ntz>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [tpep_pickup_datetime#1]\n",
      "Condition : ((isnotnull(tpep_pickup_datetime#1) AND (year(cast(tpep_pickup_datetime#1 as date)) = 2023)) AND cast(cast(tpep_pickup_datetime#1 as date) as string) IN (2023-02-14,2023-02-15,2023-02-16))\n",
      "\n",
      "(3) Project\n",
      "Output [1]: [cast(tpep_pickup_datetime#1 as date) AS p_date#86]\n",
      "Input [1]: [tpep_pickup_datetime#1]\n",
      "\n",
      "(4) HashAggregate\n",
      "Input [1]: [p_date#86]\n",
      "Keys [1]: [p_date#86]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#140L]\n",
      "Results [2]: [p_date#86, count#141L]\n",
      "\n",
      "(5) Exchange\n",
      "Input [2]: [p_date#86, count#141L]\n",
      "Arguments: hashpartitioning(p_date#86, 200), ENSURE_REQUIREMENTS, [plan_id=137]\n",
      "\n",
      "(6) HashAggregate\n",
      "Input [2]: [p_date#86, count#141L]\n",
      "Keys [1]: [p_date#86]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#134L]\n",
      "Results [2]: [p_date#86, count(1)#134L AS count(1)#135L]\n",
      "\n",
      "(7) AdaptiveSparkPlan\n",
      "Output [2]: [p_date#86, count(1)#135L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show plan\n",
    "q2b.explain(extended=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0d3603f-f4a0-4957-9e8c-b5bdf42ae5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|    p_date|count(1)|\n",
      "+----------+--------+\n",
      "|2023-01-31|       8|\n",
      "|2023-02-01|  107770|\n",
      "|2023-02-02|  113074|\n",
      "|2023-02-03|  115149|\n",
      "|2023-02-04|   98236|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+--------+\n",
      "|    p_date|count(1)|\n",
      "+----------+--------+\n",
      "|2023-01-31|       8|\n",
      "|2023-02-01|  107770|\n",
      "|2023-02-02|  113074|\n",
      "|2023-02-03|  115149|\n",
      "|2023-02-04|   98236|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+--------+\n",
      "|    p_date|count(1)|\n",
      "+----------+--------+\n",
      "|2023-01-31|       8|\n",
      "|2023-02-01|  107770|\n",
      "|2023-02-02|  113074|\n",
      "|2023-02-03|  115149|\n",
      "|2023-02-04|   98236|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+--------+\n",
      "|    p_date|count(1)|\n",
      "+----------+--------+\n",
      "|2023-01-31|       8|\n",
      "|2023-02-01|  107770|\n",
      "|2023-02-02|  113074|\n",
      "|2023-02-03|  115149|\n",
      "|2023-02-04|   98236|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+--------+\n",
      "|    p_date|count(1)|\n",
      "+----------+--------+\n",
      "|2023-01-31|       8|\n",
      "|2023-02-01|  107770|\n",
      "|2023-02-02|  113074|\n",
      "|2023-02-03|  115149|\n",
      "|2023-02-04|   98236|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+--------+\n",
      "|    p_date|count(1)|\n",
      "+----------+--------+\n",
      "|2023-01-31|       8|\n",
      "|2023-02-01|  107770|\n",
      "|2023-02-02|  113074|\n",
      "|2023-02-03|  115149|\n",
      "|2023-02-04|   98236|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+--------+\n",
      "|    p_date|count(1)|\n",
      "+----------+--------+\n",
      "|2023-01-31|       8|\n",
      "|2023-02-01|  107770|\n",
      "|2023-02-02|  113074|\n",
      "|2023-02-03|  115149|\n",
      "|2023-02-04|   98236|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+--------+\n",
      "|    p_date|count(1)|\n",
      "+----------+--------+\n",
      "|2023-01-31|       8|\n",
      "|2023-02-01|  107770|\n",
      "|2023-02-02|  113074|\n",
      "|2023-02-03|  115149|\n",
      "|2023-02-04|   98236|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "514 ms ± 147 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "# Query by partition Key; i.e. using '2023-02-14' as filter\n",
    "spark.sql(\"select p_date,count(1) from tbl_taxis_daily_raw group by p_date order by to_date(p_date)\").show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc3f508c-2f2e-40b3-a512-26bf509e8541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|    p_date|count(1)|\n",
      "+----------+--------+\n",
      "|2023-01-31|       8|\n",
      "|2023-02-01|  107770|\n",
      "|2023-02-02|  113074|\n",
      "|2023-02-03|  115149|\n",
      "|2023-02-04|   98236|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|    p_date|count(1)|\n",
      "+----------+--------+\n",
      "|2023-01-31|       8|\n",
      "|2023-02-01|  107770|\n",
      "|2023-02-02|  113074|\n",
      "|2023-02-03|  115149|\n",
      "|2023-02-04|   98236|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|    p_date|count(1)|\n",
      "+----------+--------+\n",
      "|2023-01-31|       8|\n",
      "|2023-02-01|  107770|\n",
      "|2023-02-02|  113074|\n",
      "|2023-02-03|  115149|\n",
      "|2023-02-04|   98236|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|    p_date|count(1)|\n",
      "+----------+--------+\n",
      "|2023-01-31|       8|\n",
      "|2023-02-01|  107770|\n",
      "|2023-02-02|  113074|\n",
      "|2023-02-03|  115149|\n",
      "|2023-02-04|   98236|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|    p_date|count(1)|\n",
      "+----------+--------+\n",
      "|2023-01-31|       8|\n",
      "|2023-02-01|  107770|\n",
      "|2023-02-02|  113074|\n",
      "|2023-02-03|  115149|\n",
      "|2023-02-04|   98236|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|    p_date|count(1)|\n",
      "+----------+--------+\n",
      "|2023-01-31|       8|\n",
      "|2023-02-01|  107770|\n",
      "|2023-02-02|  113074|\n",
      "|2023-02-03|  115149|\n",
      "|2023-02-04|   98236|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|    p_date|count(1)|\n",
      "+----------+--------+\n",
      "|2023-01-31|       8|\n",
      "|2023-02-01|  107770|\n",
      "|2023-02-02|  113074|\n",
      "|2023-02-03|  115149|\n",
      "|2023-02-04|   98236|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|    p_date|count(1)|\n",
      "+----------+--------+\n",
      "|2023-01-31|       8|\n",
      "|2023-02-01|  107770|\n",
      "|2023-02-02|  113074|\n",
      "|2023-02-03|  115149|\n",
      "|2023-02-04|   98236|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "1.83 s ± 330 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/21 08:30:08 WARN Executor: Issue communicating with driver in heartbeater \n",
      "org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.0.2.15:42587 in 10000 milliseconds. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat scala.util.Failure.recover(Try.scala:234)\n",
      "\tat scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.tryFailure(Promise.scala:112)\n",
      "\tat scala.concurrent.Promise.tryFailure$(Promise.scala:112)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:264)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.0.2.15:42587 in 10000 milliseconds\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:265)\n",
      "\t... 7 more\n",
      "24/03/21 08:30:08 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "# Query by partition Key; i.e. using '2023-02-14' as filter\n",
    "spark.sql(\"select p_date,count(1) from tbl_taxis_nopartitioned_raw group by p_date order by to_date(p_date)\").show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc08a75-e041-4d18-9b44-c115af373868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the session\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
